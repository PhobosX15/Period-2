{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_df = pd.read_pickle('dataset/dataframe_restaurant_filtered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_df = restaurants_df[[\"User_id\",\"Business_id\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Business_id</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2440314</th>\n",
       "      <td>1987824</td>\n",
       "      <td>45676</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588099</th>\n",
       "      <td>1987824</td>\n",
       "      <td>6663</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878432</th>\n",
       "      <td>1987824</td>\n",
       "      <td>48299</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692698</th>\n",
       "      <td>1987824</td>\n",
       "      <td>45044</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081843</th>\n",
       "      <td>1987824</td>\n",
       "      <td>23498</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         User_id Business_id Rating\n",
       "2440314  1987824       45676    5.0\n",
       "1588099  1987824        6663    4.0\n",
       "2878432  1987824       48299    4.0\n",
       "2692698  1987824       45044    5.0\n",
       "2081843  1987824       23498    5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Assuming restaurants_df has columns: 'User_id', 'Business_id', 'Rating'\n",
    "\n",
    "# Map unique user and business IDs to integer indices\n",
    "user_mapper = {user_id: index for index, user_id in enumerate(restaurants_df['User_id'].unique())}\n",
    "business_mapper = {business_id: index for index, business_id in enumerate(restaurants_df['Business_id'].unique())}\n",
    "\n",
    "# Create integer indices for users and businesses in the DataFrame\n",
    "row_indices = restaurants_df['User_id'].map(user_mapper)\n",
    "col_indices = restaurants_df['Business_id'].map(business_mapper)\n",
    "\n",
    "# Create the sparse matrix using CSR format\n",
    "ratings = sp.csr_matrix((restaurants_df['Rating'], (row_indices, col_indices)),\n",
    "                        shape=(len(user_mapper), len(business_mapper)), dtype=np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access rating use `ratings[user_mapper[user_id],business_mapper[restaurant_id]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is very sparse \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for number of latent factors might prove useful: 5-20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users= ratings.shape[0]\n",
    "num_items= ratings.shape[1]\n",
    "latent_dim= 5\n",
    "lr= 0.001\n",
    "l2_reg= 0.05\n",
    "epochs= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize user and item matrices\n",
    "user_matrix = np.random.normal(scale=1./latent_dim, size=(num_users, latent_dim))\n",
    "item_matrix = np.random.normal(scale=1./latent_dim, size=(num_items, latent_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the PMF with Hinge Loss and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSR_to_chunks(data, batchsize):\n",
    "    num_batches = data.shape[0] // batchsize\n",
    "    for i in range(num_batches):\n",
    "        yield data[i * batchsize : (i + 1) * batchsize, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute hinge loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_hinge_loss(prediction, actual):\n",
    "    loss = F.hinge_embedding_loss(prediction.view(-1), actual.view(-1), margin=1.0, reduction='mean')\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PMF(data,user_matrix,item_matrix,epochs,lr,l2_reg):\n",
    "    user_matrix = torch.tensor(user_matrix, requires_grad=True)\n",
    "    item_matrix = torch.tensor(item_matrix, requires_grad=True)\n",
    "\n",
    "    optim = torch.optim.Adam([user_matrix, item_matrix], lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_size= 1000\n",
    "        idx=0\n",
    "        for batch in CSR_to_chunks(data, batch_size):\n",
    "            optim.zero_grad()\n",
    "            shape= batch.shape[0]\n",
    "            users= user_matrix[idx:idx+shape]\n",
    "            prediction = torch.mm(users, item_matrix.t())\n",
    "            loss = compute_hinge_loss(prediction, torch.tensor(batch.todense()))\n",
    "            total_loss = loss + l2_reg * (torch.norm(users) + torch.norm(item_matrix))\n",
    "            idx+=shape\n",
    "            total_loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\pmf.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_PMF(ratings, user_matrix, item_matrix, epochs, lr, l2_reg)\n",
      "\u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\pmf.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m users\u001b[39m=\u001b[39m user_matrix[idx:idx\u001b[39m+\u001b[39mshape]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m prediction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(users, item_matrix\u001b[39m.\u001b[39mt())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m compute_hinge_loss(prediction, torch\u001b[39m.\u001b[39;49mtensor(batch\u001b[39m.\u001b[39;49mtodense()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m total_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m l2_reg \u001b[39m*\u001b[39m (torch\u001b[39m.\u001b[39mnorm(users) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mnorm(item_matrix))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m idx\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mshape\n",
      "\u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\pmf.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_hinge_loss\u001b[39m(prediction, actual):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mhinge_embedding_loss(prediction\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), actual\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), margin\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DSAI-22-24/KTH-Year2/Period-2/ResearchMethodology/Period-2/pmf.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\torch\\nn\\functional.py:3396\u001b[0m, in \u001b[0;36mhinge_embedding_loss\u001b[1;34m(input, target, margin, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3395\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 3396\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mhinge_embedding_loss(\u001b[39minput\u001b[39;49m, target, margin, reduction_enum)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_PMF(ratings, user_matrix, item_matrix, epochs, lr, l2_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
