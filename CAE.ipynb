{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Auto Encoder to generate representaion for the users and item.\n",
    "Based on the papers, `PixelCAE` and `Joint Deep Modeling of Users and Items Using Reviews for Recommendation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle load user_dataset, business_dataset, user_lookup, business_lookup\n",
    "with open('user_dataset.pkl', 'rb') as f:   \n",
    "    user_dataset = pickle.load(f)\n",
    "\n",
    "with open('business_dataset.pkl', 'rb') as f:\n",
    "    business_dataset = pickle.load(f)\n",
    "\n",
    "with open('user_lookup.pkl', 'rb') as f:\n",
    "    user_lookup = pickle.load(f)\n",
    "\n",
    "with open('business_lookup.pkl', 'rb') as f:\n",
    "    business_lookup = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Word2Vec model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Word2Vec model...\")\n",
    "from gensim.models import Word2Vec\n",
    "loaded_model = Word2Vec.load(\"word2vec.model\")\n",
    "print(\"Word2Vec model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize_filter(sentence):\n",
    "    return [token for token in word_tokenize(sentence) if token not in stop_words \n",
    "            and token not in string.punctuation ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding of filtered sentence using average of word vectors\n",
    "def get_embedding(filtered_sentence,model, avg=1,sum=0):\n",
    "   \n",
    "\n",
    "    # get embedding of words\n",
    "    if avg:\n",
    "        embeddings = []\n",
    "        for word in filtered_sentence:\n",
    "            try:\n",
    "                embeddings.append(model.wv[word])\n",
    "            except KeyError:\n",
    "                embeddings.append(np.zeros((model.vector_size,)))\n",
    "\n",
    "        # get average of embedding of words\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        return avg_embedding\n",
    "\n",
    "    if sum:\n",
    "        embeddings = []\n",
    "        for word in filtered_sentence:\n",
    "            try:\n",
    "                embeddings.append(model.wv[word])\n",
    "            except KeyError:\n",
    "                embeddings.append(np.zeros((model.vector_size,)))\n",
    "\n",
    "        # get sum of embedding of words\n",
    "        sum_embedding = np.sum(embeddings, axis=0)\n",
    "        return sum_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new `user_dataset`, `business_dataset`, `user_lookup` and `business_lookup` we can create our Custom data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_matrix(w2vmodel, review, time_index, scaling_factor=0.2):\n",
    "    \"\"\" Given a review, generate the review matrix and apply time-decay to the review matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w2vmodel: word2vec model\n",
    "    review: review to generate the review matrix\n",
    "    time_stamp: timestamp of the review\n",
    "    scaling_factor: scaling factor for time-decay\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    review_matrix: review matrix of the review\n",
    "    \"\"\"\n",
    "    filtered_sentence = tokenize_filter(review)\n",
    "    review_matrix = get_embedding(filtered_sentence,w2vmodel)\n",
    "    # apply time-decay\n",
    "    review_matrix = review_matrix * np.exp(-scaling_factor * 10-time_index)\n",
    "    # reshape review matrix to review matrix.shape[0]/10,10\n",
    "    review_matrix = review_matrix.reshape(int(review_matrix.shape[0]/10),10)\n",
    "\n",
    "\n",
    "    return review_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_matrix(user_dataset, user_lookup, user_id, w2vmodel,):\n",
    "    \"\"\"\n",
    "    Create a user matrix for a user_id in user_dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_dataset : dataframe with latest 10 entries of users\n",
    "    business_dataset : dataframe with latest 10 entries of businesses\n",
    "    user_lookup : dict of user_id and its start and end index in user_dataset\n",
    "    business_lookup : dict of business_id and its start and end index in business_dataset\n",
    "    w2vmodel : word2vec model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    user_matrix : numpy array of shape (10,10,10)\n",
    "    \"\"\"\n",
    "    # get the start and end index of user_id in user_dataset\n",
    "    start_index, end_index = user_lookup[user_id]\n",
    "    # get the latest 10 entries of user_id\n",
    "    data = user_dataset.iloc[start_index:end_index]\n",
    "    user_matrix = [np.array(get_review_matrix(w2vmodel,review,i)) for i,review in enumerate(data.review)]\n",
    "    user_matrix = np.array(user_matrix)\n",
    "    return user_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_matrix(business_dataset,business_lookup,business_id,w2vmodel,):\n",
    "    \"\"\"\n",
    "    Create an item matrix for a business_id in business_dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    business_dataset : dataframe with latest 10 entries of businesses\n",
    "    business_lookup : dict of business_id and its start and end index in business_dataset\n",
    "    w2vmodel : word2vec model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    item_matrix : numpy array of shape (10,10,10)\n",
    "    \"\"\"\n",
    "    # get the start and end index of business_id in business_dataset\n",
    "    start_index, end_index = business_lookup[business_id]\n",
    "    # get the latest 10 entries of business_id\n",
    "    data = business_dataset.iloc[start_index:end_index]\n",
    "    item_matrix = [np.array(get_review_matrix(w2vmodel,review,i)) \n",
    "                   for i,review in enumerate(data.review)]\n",
    "    item_matrix = np.array(item_matrix)\n",
    "    return item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, user_dataset,business_dataset,user_lookup,business_lookup,\n",
    "                 w2vmodel,) -> None:\n",
    "        super().__init__()\n",
    "        self.user_dataset = user_dataset\n",
    "        self.business_dataset = business_dataset\n",
    "        self.user_lookup = user_lookup\n",
    "        self.business_lookup = business_lookup\n",
    "        self.w2vmodel = w2vmodel\n",
    "        self.items = list(self.user_lookup.items())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.items[idx][0]\n",
    "        \n",
    "        # get user_matrix \n",
    "        user_matrix = get_user_matrix(self.user_dataset, self.user_lookup, user_id, self.w2vmodel)\n",
    "        data = self.user_dataset.iloc[user_lookup[user_id][0]:user_lookup[user_id][1]]\n",
    "        # get item_matrix for each business_id\n",
    "        item_matrix = []\n",
    "        for business_id in data.business_id:\n",
    "            item_matrix.append(get_item_matrix(self.business_dataset, self.business_lookup, business_id, self.w2vmodel))\n",
    "        item_matrix = np.array(item_matrix)\n",
    "        return user_matrix, item_matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserDataset(Dataset):\n",
    "    def __init__(self, user_dataset,user_ids,user_lookup,w2vmodel):\n",
    "        super().__init__()\n",
    "        self.user_dataset = user_dataset\n",
    "        self.user_ids = user_ids\n",
    "        self.user_lookup = user_lookup\n",
    "        self.w2vmodel = w2vmodel\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx]\n",
    "        \n",
    "        # get user_matrix \n",
    "        user_matrix = get_user_matrix(self.user_dataset, self.user_lookup, user_id, self.w2vmodel)\n",
    "        return user_matrix, user_id\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessDataset(Dataset):\n",
    "    def __init__(self, business_dataset,business_ids,business_lookup,w2vmodel):\n",
    "        super().__init__()\n",
    "        self.business_dataset = business_dataset\n",
    "        self.business_lookup = business_lookup\n",
    "        self.w2vmodel = w2vmodel\n",
    "        self.business_ids = business_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.business_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        business_id = self.business_ids[idx]\n",
    "        # get item_matrix for each business_id\n",
    "        item_matrix = get_item_matrix(self.business_dataset, self.business_lookup, business_id, self.w2vmodel)\n",
    "        return item_matrix, business_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data in `Train 80%`, `Test 10%` and `Validate 10%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_and_loaders(dataset, lookup, model, kind, batch_size=64):\n",
    "    num_items = len(lookup)\n",
    "    ids = list(lookup.keys())\n",
    "\n",
    "    train_size = int(0.8 * num_items)\n",
    "    validation_size = int(0.1 * num_items)\n",
    "    test_size = num_items - train_size - validation_size\n",
    "\n",
    "    train_start_index = 0\n",
    "    train_end_index = train_size\n",
    "\n",
    "    validation_start_index = train_size\n",
    "    validation_end_index = train_size + validation_size\n",
    "\n",
    "    test_start_index = train_size + validation_size\n",
    "    test_end_index = num_items\n",
    "\n",
    "    if kind == \"user\":\n",
    "        train_dataset       = UserDataset(dataset, ids[train_start_index:train_end_index],\n",
    "                                           lookup, model)\n",
    "        validation_dataset  = UserDataset(dataset, ids[validation_start_index:validation_end_index],\n",
    "                                           lookup, model)\n",
    "        test_dataset        = UserDataset(dataset, ids[test_start_index:test_end_index],\n",
    "                                           lookup, model)\n",
    "\n",
    "    elif kind == \"business\":\n",
    "        train_dataset       = BusinessDataset(dataset, ids[train_start_index:train_end_index],\n",
    "                                               lookup, model)\n",
    "        validation_dataset  = BusinessDataset(dataset, ids[validation_start_index:validation_end_index],\n",
    "                                               lookup, model)\n",
    "        test_dataset        = BusinessDataset(dataset, ids[test_start_index:test_end_index],\n",
    "                                               lookup, model)\n",
    "\n",
    "    train_dataloader        = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "    validation_dataloader   = DataLoader(validation_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "    test_dataloader         = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a Convolutional Auto Encoder to learn represnetation of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_CAE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(user_CAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(10, 32, kernel_size=2 if input_dim[0]==5 else 5, stride=1,\n",
    "                       padding=2 if input_dim[0] == 20 else 1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2 if input_dim[0]==5 else 5, stride=1,\n",
    "                       padding=2 if input_dim[0] == 20 else 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32,64, kernel_size=2, stride=1,\n",
    "                       padding=0 if input_dim[0] == 20 else 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64, kernel_size=2, stride=1,\n",
    "                       padding=0 if input_dim[0] == 20 else (1 if input_dim[0]==10 else (0,1))),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 100),\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # reshape according to the shape of the input\n",
    "        x = self.encoder(x)\n",
    "        x = x.reshape(x.shape[0],10*10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_decoder(nn.Module):\n",
    "    def __init__(self,output_dim):\n",
    "        super(user_decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 2, 2)),  # reverse of Flatten\n",
    "            nn.ConvTranspose2d(64, 64,\n",
    "                                kernel_size=4 if output_dim[0]==10 else ((8,4) if output_dim[0]==20 else (2,4)),\n",
    "                                stride=2, padding=1),  # reverse of MaxPool2d\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2 , stride=1, padding= 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, \n",
    "                               kernel_size=3 if output_dim[0]==10 else ((8,6) if output_dim[0]==20 else (4,6)), \n",
    "                               stride=2, padding=0 if output_dim[0]==10 else 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, \n",
    "                      kernel_size=2 if output_dim[0]==10 else ((3,3) if output_dim[0]==20 else (2,3)), \n",
    "                      stride=1, padding=1 if output_dim[0]==10 else (0)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,10,kernel_size=1,stride=1,padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(x.shape[0],10,*self.output_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class item_CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(item_CAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(10, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32,64, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 100),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedding = torch.empty(0).cuda()\n",
    "        for relevant_item in x:\n",
    "            for item in relevant_item:\n",
    "                item = item.unsqueeze(0)\n",
    "                embedding = torch.cat((embedding, self.encoder(item)), 0)\n",
    "        \n",
    "        # Apply weighted time decay and average pooling to the embeddings\n",
    "        embedding = embedding.reshape(64,10,100)\n",
    "        embedding = embedding * torch.exp(-0.2 * 10 - torch.arange(10)).unsqueeze(0).unsqueeze(-1).cuda()\n",
    "        embedding = torch.mean(embedding, 1)\n",
    "        return embedding\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class item_decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(item_decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 2, 2)),  # reverse of Flatten\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),  # reverse of MaxPool2d\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,10,kernel_size=1,stride=1,padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings= torch.empty(0).cuda()\n",
    "        for item in x:\n",
    "            embeddings = torch.cat((embeddings, self.decoder(item)), 0)\n",
    "    \n",
    "        embeddings = embeddings.reshape(64,10,10,10)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. Based on reconstrution loss and regularization terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss_with_regularization(x, y, model, lambda_l1=0.01):\n",
    "    \"\"\"Compute reconstruction loss with L1 regularization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : noisy input\n",
    "    y : clean input\n",
    "    model : autoencoder model\n",
    "    lambda_l1 : regularization parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : reconstruction loss with L1 regularization as tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute MSE\n",
    "    mse_loss = nn.MSELoss()\n",
    "    mse = mse_loss(x, y)\n",
    "\n",
    "    # Compute L1 regularization\n",
    "    l1_reg = torch.tensor(0., requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "    \n",
    "    # Combine MSE and L1 regularization\n",
    "    loss = mse + lambda_l1 * l1_reg\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_decoder(\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Unflatten(dim=1, unflattened_size=(64, 2, 2))\n",
       "    (5): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_encoder_instance = user_CAE()\n",
    "user_decoder_instance = user_decoder()\n",
    "\"\"\" \n",
    "item_encoder_instance = item_CAE()\n",
    "item_decoder_instance = item_decoder() \"\"\"\n",
    "\n",
    "item_encoder_instance = user_CAE()\n",
    "item_decoder_instance = user_decoder()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "user_encoder_instance.to(device)\n",
    "user_decoder_instance.to(device)\n",
    "\n",
    "item_encoder_instance.to(device)\n",
    "item_decoder_instance.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/172 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 172/172 [06:16<00:00,  2.19s/it]\n",
      "Epoch 1/10 [Validate]: 100%|██████████| 22/22 [00:39<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg Training loss: 3.69585053276184, Avg Validation loss: 0.5642752159725536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 172/172 [05:46<00:00,  2.02s/it]\n",
      "Epoch 2/10 [Validate]: 100%|██████████| 22/22 [00:41<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg Training loss: 0.542634176653485, Avg Validation loss: 0.5200417339801788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 172/172 [05:43<00:00,  2.00s/it]\n",
      "Epoch 3/10 [Validate]: 100%|██████████| 22/22 [00:40<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg Training loss: 0.5226871378199999, Avg Validation loss: 0.5132199769670313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]:  23%|██▎       | 40/172 [01:22<04:31,  2.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m total_train_variance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_matrix \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     18\u001b[0m     user_matrix \u001b[38;5;241m=\u001b[39m user_matrix\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Compute variance of user_matrix\u001b[39;00m\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[61], line 17\u001b[0m, in \u001b[0;36mUserDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ids[idx]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# get user_matrix \u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m user_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_user_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_lookup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2vmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_matrix\n",
      "Cell \u001b[1;32mIn[44], line 21\u001b[0m, in \u001b[0;36mget_user_matrix\u001b[1;34m(user_dataset, user_lookup, user_id, w2vmodel)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# get the latest 10 entries of user_id\u001b[39;00m\n\u001b[0;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m user_dataset\u001b[38;5;241m.\u001b[39miloc[start_index:end_index]\n\u001b[1;32m---> 21\u001b[0m user_matrix \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(get_review_matrix(w2vmodel,review,i)) \u001b[38;5;28;01mfor\u001b[39;00m i,review \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data\u001b[38;5;241m.\u001b[39mreview)]\n\u001b[0;32m     22\u001b[0m user_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(user_matrix)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_matrix\n",
      "Cell \u001b[1;32mIn[44], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# get the latest 10 entries of user_id\u001b[39;00m\n\u001b[0;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m user_dataset\u001b[38;5;241m.\u001b[39miloc[start_index:end_index]\n\u001b[1;32m---> 21\u001b[0m user_matrix \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mget_review_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2vmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i,review \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data\u001b[38;5;241m.\u001b[39mreview)]\n\u001b[0;32m     22\u001b[0m user_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(user_matrix)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_matrix\n",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m, in \u001b[0;36mget_review_matrix\u001b[1;34m(w2vmodel, review, time_index, scaling_factor)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Given a review, generate the review matrix and apply time-decay to the review matrix\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mreview_matrix: review matrix of the review\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m filtered_sentence \u001b[38;5;241m=\u001b[39m tokenize_filter(review)\n\u001b[1;32m---> 16\u001b[0m review_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2vmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# apply time-decay\u001b[39;00m\n\u001b[0;32m     18\u001b[0m review_matrix \u001b[38;5;241m=\u001b[39m review_matrix \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mscaling_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m-\u001b[39mtime_index)\n",
      "Cell \u001b[1;32mIn[42], line 9\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(filtered_sentence, model, avg, sum)\u001b[0m\n\u001b[0;32m      7\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_sentence]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# get average of embedding of words\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     avg_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_embedding\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m:\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\DSAI-22-24\\KTH-Year2\\Period-2\\ResearchMethodology\\Period-2\\.conda\\lib\\site-packages\\numpy\\core\\_methods.py:102\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 102\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     rcount \u001b[38;5;241m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "user_encoder_instance.apply(init_weights)\n",
    "user_decoder_instance.apply(init_weights)\n",
    "\n",
    "user_optimizer = torch.optim.RMSprop(list(user_encoder_instance.parameters()) + list(user_decoder_instance.parameters()), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "epochs = 10\n",
    "best_user_loss = float('inf')  # Initialize with a high value\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    pbar = tqdm(user_train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    total_train_loss = 0\n",
    "    total_train_variance = 0\n",
    "    for user_matrix in pbar:\n",
    "        user_matrix = user_matrix.to(device)\n",
    "\n",
    "        # Compute variance of user_matrix\n",
    "        variance = user_matrix.var()\n",
    "        total_train_variance += variance.item()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        user_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        user_embedding = user_encoder_instance(user_matrix)\n",
    "        user_output = user_decoder_instance(user_embedding)\n",
    "        user_loss = reconstruction_loss_with_regularization(user_output, user_matrix, user_encoder_instance)\n",
    "        total_train_loss += user_loss.item()\n",
    "\n",
    "        user_loss.backward()\n",
    "        user_optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(user_train_dataloader)\n",
    "    avg_train_variance = total_train_variance / len(user_train_dataloader)\n",
    "    pbar.set_postfix({\"Avg Training loss\": avg_train_loss, \"Avg Input variance\": avg_train_variance})\n",
    "\n",
    "    # Validation phase\n",
    "    pbar = tqdm(user_validation_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validate]\")\n",
    "    total_validation_loss = 0\n",
    "    total_validation_variance = 0\n",
    "    with torch.no_grad():\n",
    "        for user_matrix in pbar:\n",
    "            user_matrix = user_matrix.to(device)\n",
    "\n",
    "            # Compute variance of user_matrix\n",
    "            variance = user_matrix.var()\n",
    "            total_validation_variance += variance.item()\n",
    "\n",
    "            # forward\n",
    "            user_embedding = user_encoder_instance(user_matrix)\n",
    "            user_output = user_decoder_instance(user_embedding)\n",
    "            user_loss = reconstruction_loss_with_regularization(user_output, user_matrix, user_encoder_instance)\n",
    "            total_validation_loss += user_loss.item()\n",
    "\n",
    "    avg_validation_loss = total_validation_loss / len(user_validation_dataloader)\n",
    "    avg_validation_variance = total_validation_variance / len(user_validation_dataloader)\n",
    "    pbar.set_postfix({\"Avg Validation loss\": avg_validation_loss, \"Avg Input variance\": avg_validation_variance})\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Avg Training loss: {avg_train_loss}, Avg Validation loss: {avg_validation_loss}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_validation_loss < best_user_loss:\n",
    "        best_user_loss = avg_validation_loss\n",
    "        torch.save(user_encoder_instance.state_dict(), \"best_user_encoder_model.pth\")\n",
    "        torch.save(user_decoder_instance.state_dict(), \"best_user_decoder_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder_instance, decoder_instance, train_dataloader, validation_dataloader, device, name,epochs=10):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    encoder_instance.apply(init_weights)\n",
    "    decoder_instance.apply(init_weights)\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(list(encoder_instance.parameters()) + list(decoder_instance.parameters()), lr=0.001)\n",
    "\n",
    "    best_loss = float('inf')  # Initialize with a high value\n",
    "    training_stats = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        total_train_loss = 0\n",
    "        total_train_variance = 0\n",
    "        epochs_no_improve = 0\n",
    "        for matrix in pbar:\n",
    "            matrix = matrix.to(device).float()\n",
    "\n",
    "            # Compute variance of matrix\n",
    "            variance = matrix.var()\n",
    "            total_train_variance += variance.item()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            embedding = encoder_instance(matrix)\n",
    "            output = decoder_instance(embedding)\n",
    "            loss = reconstruction_loss_with_regularization(output, matrix, encoder_instance)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        avg_train_variance = total_train_variance / len(train_dataloader)\n",
    "        print(f\"Avg Training loss: {avg_train_loss}, Avg Input variance: {avg_train_variance}\")\n",
    "        # Validation phase\n",
    "        pbar = tqdm(validation_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validate]\")\n",
    "        total_validation_loss = 0\n",
    "        total_validation_variance = 0\n",
    "        with torch.no_grad():\n",
    "            for matrix in pbar:\n",
    "                matrix = matrix.to(device).float()\n",
    "\n",
    "                # Compute variance of matrix\n",
    "                variance = matrix.var()\n",
    "                total_validation_variance += variance.item()\n",
    "\n",
    "                # forward\n",
    "                embedding = encoder_instance(matrix)\n",
    "                output = decoder_instance(embedding)\n",
    "                loss = reconstruction_loss_with_regularization(output, matrix, encoder_instance)\n",
    "                total_validation_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = total_validation_loss / len(validation_dataloader)\n",
    "        avg_validation_variance = total_validation_variance / len(validation_dataloader)\n",
    "        print(f\"Avg Validation loss: {avg_validation_loss}, Avg Input variance: {avg_validation_variance}\")\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Avg Training loss: {avg_train_loss}, Avg Validation loss: {avg_validation_loss}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_validation_loss < best_loss:\n",
    "            best_loss = avg_validation_loss\n",
    "            torch.save(encoder_instance.state_dict(), f\"AE_model_weights\\\\{name}_best_encoder_model.pth\")\n",
    "            torch.save(decoder_instance.state_dict(), f\"AE_model_weights\\\\{name}_best_decoder_model.pth\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == 2:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "        # Store the metrics\n",
    "        training_stats.append({\n",
    "            \"name\": name,\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_validation_loss,\n",
    "            'Training Var': avg_train_variance,\n",
    "            'Valid. Var': avg_validation_variance\n",
    "        })\n",
    "\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec_model_200_5.model...\n",
      "word2vec_model_200_5.model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 172/172 [05:38<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 3.705801231570022, Avg Input variance: 0.00032106780485645326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validate]: 100%|██████████| 22/22 [00:40<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5672207420522516, Avg Input variance: 0.00033068730871574104\n",
      "Epoch: 1, Avg Training loss: 3.705801231570022, Avg Validation loss: 0.5672207420522516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 172/172 [06:05<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5428564108388368, Avg Input variance: 0.000321116432280897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validate]: 100%|██████████| 22/22 [00:47<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5227399522607977, Avg Input variance: 0.000330227648109113\n",
      "Epoch: 2, Avg Training loss: 0.5428564108388368, Avg Validation loss: 0.5227399522607977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 172/172 [06:32<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5229091360125431, Avg Input variance: 0.00032108639949678206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validate]: 100%|██████████| 22/22 [00:46<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.515880365263332, Avg Input variance: 0.0003305195330705663\n",
      "Epoch: 3, Avg Training loss: 0.5229091360125431, Avg Validation loss: 0.515880365263332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 172/172 [06:22<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5196466186018878, Avg Input variance: 0.00032107687205330706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validate]: 100%|██████████| 22/22 [00:42<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.514691645448858, Avg Input variance: 0.00033108126213350755\n",
      "Epoch: 4, Avg Training loss: 0.5196466186018878, Avg Validation loss: 0.514691645448858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 172/172 [06:11<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5190753247155699, Avg Input variance: 0.0003211013099644333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validate]: 100%|██████████| 22/22 [00:42<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5144824087619781, Avg Input variance: 0.00033213274251796645\n",
      "Epoch: 5, Avg Training loss: 0.5190753247155699, Avg Validation loss: 0.5144824087619781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 172/172 [05:49<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5189736482015875, Avg Input variance: 0.0003210876661902402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validate]: 100%|██████████| 22/22 [00:41<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5144463371146809, Avg Input variance: 0.0003318480700148608\n",
      "Epoch: 6, Avg Training loss: 0.5189736482015875, Avg Validation loss: 0.5144463371146809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 172/172 [05:45<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5189552913571513, Avg Input variance: 0.00032110023650687273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validate]: 100%|██████████| 22/22 [00:42<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5144101137464697, Avg Input variance: 0.00033137914777564055\n",
      "Epoch: 7, Avg Training loss: 0.5189552913571513, Avg Validation loss: 0.5144101137464697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 172/172 [05:53<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5189043408216432, Avg Input variance: 0.00032110644695971727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validate]: 100%|██████████| 22/22 [00:44<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5136213383891366, Avg Input variance: 0.000330858240803619\n",
      "Epoch: 8, Avg Training loss: 0.5189043408216432, Avg Validation loss: 0.5136213383891366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 172/172 [05:48<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5189342488383137, Avg Input variance: 0.00032107378603202875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validate]: 100%|██████████| 22/22 [00:42<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5140841738744215, Avg Input variance: 0.00033075182397045535\n",
      "Epoch: 9, Avg Training loss: 0.5189342488383137, Avg Validation loss: 0.5140841738744215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 172/172 [05:46<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5189331787270169, Avg Input variance: 0.000321039785651306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validate]: 100%|██████████| 22/22 [00:40<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5140498849478635, Avg Input variance: 0.00033133701072074473\n",
      "Epoch: 10, Avg Training loss: 0.5189331787270169, Avg Validation loss: 0.5140498849478635\n",
      "Loading word2vec_model_200_10.model...\n",
      "word2vec_model_200_10.model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 172/172 [05:47<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 3.7206491228452947, Avg Input variance: 0.000494411175591373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validate]: 100%|██████████| 22/22 [00:40<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5664361986246976, Avg Input variance: 0.0005110968605467034\n",
      "Epoch: 1, Avg Training loss: 3.7206491228452947, Avg Validation loss: 0.5664361986246976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 172/172 [05:37<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5430344301600789, Avg Input variance: 0.0004944449246977456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validate]: 100%|██████████| 22/22 [00:40<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5220563520084728, Avg Input variance: 0.000510880182528953\n",
      "Epoch: 2, Avg Training loss: 0.5430344301600789, Avg Validation loss: 0.5220563520084728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 172/172 [06:12<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.523087474149327, Avg Input variance: 0.0004944494400216743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5152169113809412, Avg Input variance: 0.0005115472330627116\n",
      "Epoch: 3, Avg Training loss: 0.523087474149327, Avg Validation loss: 0.5152169113809412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 172/172 [07:05<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5198247439639513, Avg Input variance: 0.0004943610850226156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validate]: 100%|██████████| 22/22 [00:51<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5140294297174974, Avg Input variance: 0.0005096207645361904\n",
      "Epoch: 4, Avg Training loss: 0.5198247439639513, Avg Validation loss: 0.5140294297174974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 172/172 [07:09<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5192530803209128, Avg Input variance: 0.0004944160778947218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5138226774605837, Avg Input variance: 0.0005113714300519364\n",
      "Epoch: 5, Avg Training loss: 0.5192530803209128, Avg Validation loss: 0.5138226774605837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 172/172 [07:09<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5191518757925477, Avg Input variance: 0.0004943960070810389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5137845277786255, Avg Input variance: 0.0005103122808081521\n",
      "Epoch: 6, Avg Training loss: 0.5191518757925477, Avg Validation loss: 0.5137845277786255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 172/172 [07:03<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5191333876099697, Avg Input variance: 0.0004944489183524342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5137398486787622, Avg Input variance: 0.0005108991701325232\n",
      "Epoch: 7, Avg Training loss: 0.5191333876099697, Avg Validation loss: 0.5137398486787622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 172/172 [07:03<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5190902584513952, Avg Input variance: 0.0004943795380355834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5161042511463165, Avg Input variance: 0.0005110770897855135\n",
      "Epoch: 8, Avg Training loss: 0.5190902584513952, Avg Validation loss: 0.5161042511463165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 172/172 [07:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5191307507975157, Avg Input variance: 0.0004944321496390499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5164426836100492, Avg Input variance: 0.0005108372332126072\n",
      "Epoch: 9, Avg Training loss: 0.5191307507975157, Avg Validation loss: 0.5164426836100492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 172/172 [07:06<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.5191305491120316, Avg Input variance: 0.0004944434948874702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validate]: 100%|██████████| 22/22 [00:50<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.5165692296895114, Avg Input variance: 0.0005114381691038778\n",
      "Epoch: 10, Avg Training loss: 0.5191305491120316, Avg Validation loss: 0.5165692296895114\n",
      "Loading word2vec_model_50_5.model...\n",
      "word2vec_model_50_5.model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 172/172 [07:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 3.628225565996281, Avg Input variance: 0.001265217014046949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validate]: 100%|██████████| 22/22 [00:49<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.4129491001367569, Avg Input variance: 0.0012916530672968788\n",
      "Epoch: 1, Avg Training loss: 3.628225565996281, Avg Validation loss: 0.4129491001367569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 172/172 [07:08<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3958437730406606, Avg Input variance: 0.0012651991463255483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validate]: 100%|██████████| 22/22 [00:49<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.38063851676203986, Avg Input variance: 0.0012961009951223705\n",
      "Epoch: 2, Avg Training loss: 0.3958437730406606, Avg Validation loss: 0.38063851676203986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 172/172 [07:00<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.38132787565159243, Avg Input variance: 0.001265137966689762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validate]: 100%|██████████| 22/22 [00:49<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37566492232409393, Avg Input variance: 0.0012947704156183383\n",
      "Epoch: 3, Avg Training loss: 0.38132787565159243, Avg Validation loss: 0.37566492232409393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 172/172 [07:05<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3789527203107989, Avg Input variance: 0.0012652350821109966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validate]: 100%|██████████| 22/22 [00:52<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3747992745854638, Avg Input variance: 0.0012992639570835638\n",
      "Epoch: 4, Avg Training loss: 0.3789527203107989, Avg Validation loss: 0.3747992745854638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 172/172 [07:03<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3785365717702134, Avg Input variance: 0.0012651670529225537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validate]: 100%|██████████| 22/22 [00:49<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3746426525441083, Avg Input variance: 0.0012939565825614739\n",
      "Epoch: 5, Avg Training loss: 0.3785365717702134, Avg Validation loss: 0.3746426525441083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 172/172 [06:54<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3784627455265023, Avg Input variance: 0.001265350878807218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validate]: 100%|██████████| 22/22 [00:48<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3746159781109203, Avg Input variance: 0.0012935549073683267\n",
      "Epoch: 6, Avg Training loss: 0.3784627455265023, Avg Validation loss: 0.3746159781109203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 172/172 [07:00<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3784488391044528, Avg Input variance: 0.0012652061014641943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validate]: 100%|██████████| 22/22 [00:49<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37462595105171204, Avg Input variance: 0.0012962959907864306\n",
      "Epoch: 7, Avg Training loss: 0.3784488391044528, Avg Validation loss: 0.37462595105171204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 172/172 [06:50<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3784133434988732, Avg Input variance: 0.0012652155493797605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validate]: 100%|██████████| 22/22 [00:49<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37415470860221167, Avg Input variance: 0.0012928726342083378\n",
      "Epoch: 8, Avg Training loss: 0.3784133434988732, Avg Validation loss: 0.37415470860221167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 172/172 [07:31<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.37844726703194687, Avg Input variance: 0.001265137959244596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validate]: 100%|██████████| 22/22 [00:58<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3736256279728629, Avg Input variance: 0.0012962978798896074\n",
      "Epoch: 9, Avg Training loss: 0.37844726703194687, Avg Validation loss: 0.3736256279728629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 172/172 [08:09<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.378446169544098, Avg Input variance: 0.0012652562196139073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validate]: 100%|██████████| 22/22 [00:55<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37388110567222943, Avg Input variance: 0.0012899333325384016\n",
      "Epoch: 10, Avg Training loss: 0.378446169544098, Avg Validation loss: 0.37388110567222943\n",
      "Loading word2vec_model_50_10.model...\n",
      "word2vec_model_50_10.model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 172/172 [08:06<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 3.633414086387601, Avg Input variance: 0.0019161451403538934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validate]: 100%|██████████| 22/22 [00:57<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.41547739099372516, Avg Input variance: 0.0019598463285629723\n",
      "Epoch: 1, Avg Training loss: 3.633414086387601, Avg Validation loss: 0.41547739099372516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 172/172 [08:09<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3964935671104941, Avg Input variance: 0.0019161233869327103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validate]: 100%|██████████| 22/22 [00:55<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3830977149985053, Avg Input variance: 0.0019606967477805233\n",
      "Epoch: 2, Avg Training loss: 0.3964935671104941, Avg Validation loss: 0.3830977149985053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 172/172 [07:24<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.38197629361651664, Avg Input variance: 0.0019162573816449663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37809842689470813, Avg Input variance: 0.001959797492335466\n",
      "Epoch: 3, Avg Training loss: 0.38197629361651664, Avg Validation loss: 0.37809842689470813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 172/172 [08:59<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.379601638504239, Avg Input variance: 0.0019163978374075838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3772396729751067, Avg Input variance: 0.001973788391544738\n",
      "Epoch: 4, Avg Training loss: 0.379601638504239, Avg Validation loss: 0.3772396729751067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 172/172 [08:40<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.37918488785277965, Avg Input variance: 0.0019161905362379066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3770838691429658, Avg Input variance: 0.0019685930508950896\n",
      "Epoch: 5, Avg Training loss: 0.37918488785277965, Avg Validation loss: 0.3770838691429658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 172/172 [08:40<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.37911079512086027, Avg Input variance: 0.0019162847805324248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37706200913949445, Avg Input variance: 0.0019697675280357626\n",
      "Epoch: 6, Avg Training loss: 0.37911079512086027, Avg Validation loss: 0.37706200913949445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 172/172 [08:39<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3790971054587253, Avg Input variance: 0.0019162566878908683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3770960122346878, Avg Input variance: 0.0019707258854230695\n",
      "Epoch: 7, Avg Training loss: 0.3790971054587253, Avg Validation loss: 0.3770960122346878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 172/172 [08:39<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.37902588969053225, Avg Input variance: 0.0019164995918440264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.38135168091817334, Avg Input variance: 0.001966949610505253\n",
      "Epoch: 8, Avg Training loss: 0.37902588969053225, Avg Validation loss: 0.38135168091817334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 172/172 [08:39<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3790941527763078, Avg Input variance: 0.0019163395085935156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.382068085399541, Avg Input variance: 0.001963627937419171\n",
      "Epoch: 9, Avg Training loss: 0.3790941527763078, Avg Validation loss: 0.382068085399541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 172/172 [08:39<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3790949761520985, Avg Input variance: 0.0019161932442478056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validate]: 100%|██████████| 22/22 [01:01<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3821541545065967, Avg Input variance: 0.001968536139677533\n",
      "Epoch: 10, Avg Training loss: 0.3790949761520985, Avg Validation loss: 0.3821541545065967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test which of the 6 w2v models performs best\n",
    "\n",
    "# get the list of files in the directory\n",
    "import os\n",
    "files = [(\"word2vec_model_200_5.model\",(20,10)),(\"word2vec_model_200_10.model\",(20,10)),\n",
    "         (\"word2vec_model_50_5.model\",(5,10)),(\"word2vec_model_50_10.model\",(5,10)),]\n",
    "\n",
    "\n",
    "\n",
    "# iterate through the list of files\n",
    "for file, dimension in files:\n",
    "\n",
    "    file_name=file.split('.')[0]\n",
    "    # load the model\n",
    "    print(f\"Loading {file}...\")\n",
    "    loaded_model = Word2Vec.load(f\"word2vec_model/{file}\")\n",
    "    print(f\"{file} loaded.\")\n",
    "\n",
    "    # get the shape of the embedding\n",
    "    embedding_dim = loaded_model.wv.vector_size\n",
    "\n",
    "    # create dataset and dataloader\n",
    "    user_train_dataloader, user_validation_dataloader, user_test_dataloader = create_datasets_and_loaders(user_dataset, user_lookup, loaded_model, \"user\", batch_size=64)\n",
    "\n",
    "    # create model instances\n",
    "    user_encoder_instance = user_CAE(input_dim = dimension)\n",
    "    user_decoder_instance = user_decoder(output_dim = dimension)\n",
    "\n",
    "    # move the model to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    user_encoder_instance.to(device)\n",
    "    user_decoder_instance.to(device)\n",
    "\n",
    "    # train the model\n",
    "    training_stats = train_model(user_encoder_instance, user_decoder_instance, user_train_dataloader, user_validation_dataloader, device, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'word2vec_model_100_5',\n",
       "  'epoch': 1,\n",
       "  'Training Loss': 3.702573890602866,\n",
       "  'Valid. Loss': 0.5644270290027965,\n",
       "  'Training Var': 0.0006341415519696162,\n",
       "  'Valid. Var': 0.0006514405567114326},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 2,\n",
       "  'Training Loss': 0.5430761939564417,\n",
       "  'Valid. Loss': 0.5202192041006956,\n",
       "  'Training Var': 0.0006341679389913414,\n",
       "  'Valid. Var': 0.0006524893529289825},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 3,\n",
       "  'Training Loss': 0.5231285705122837,\n",
       "  'Valid. Loss': 0.5133956670761108,\n",
       "  'Training Var': 0.0006342082491501906,\n",
       "  'Valid. Var': 0.0006500724306203086},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 4,\n",
       "  'Training Loss': 0.5198649736337884,\n",
       "  'Valid. Loss': 0.5122098218310963,\n",
       "  'Training Var': 0.000634200663541357,\n",
       "  'Valid. Var': 0.0006513336451131512},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 5,\n",
       "  'Training Loss': 0.5192937258371088,\n",
       "  'Valid. Loss': 0.5120027552951466,\n",
       "  'Training Var': 0.0006342344392134353,\n",
       "  'Valid. Var': 0.0006540099063634195},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 6,\n",
       "  'Training Loss': 0.5191921009573826,\n",
       "  'Valid. Loss': 0.5119697749614716,\n",
       "  'Training Var': 0.0006342515360217455,\n",
       "  'Valid. Var': 0.0006531436837659302},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 7,\n",
       "  'Training Loss': 0.5191739769869073,\n",
       "  'Valid. Loss': 0.5119697424498472,\n",
       "  'Training Var': 0.0006342024669635954,\n",
       "  'Valid. Var': 0.0006525598926766014},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 8,\n",
       "  'Training Loss': 0.5191537048234496,\n",
       "  'Valid. Loss': 0.5124433284456079,\n",
       "  'Training Var': 0.0006342041949189246,\n",
       "  'Valid. Var': 0.00065022516223094},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 9,\n",
       "  'Training Loss': 0.5191718554773996,\n",
       "  'Valid. Loss': 0.5123424963517622,\n",
       "  'Training Var': 0.0006341609720081135,\n",
       "  'Valid. Var': 0.0006526908678510649},\n",
       " {'name': 'word2vec_model_100_5',\n",
       "  'epoch': 10,\n",
       "  'Training Loss': 0.5191702489242997,\n",
       "  'Valid. Loss': 0.5124106082049283,\n",
       "  'Training Var': 0.0006341732775137304,\n",
       "  'Valid. Var': 0.0006517684801523997}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the item embedding generator using the Business Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 67/67 [06:38<00:00,  5.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 8.57864819889638, Avg Input variance: 0.0019937266731189923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Validate]: 100%|██████████| 9/9 [00:54<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.579171982076433, Avg Input variance: 0.0019558508259554705\n",
      "Epoch: 1, Avg Training loss: 8.57864819889638, Avg Validation loss: 0.579171982076433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 67/67 [06:42<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.4843931060228775, Avg Input variance: 0.0019918647368869454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Validate]: 100%|██████████| 9/9 [00:55<00:00,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.43669845660527545, Avg Input variance: 0.0019546463283606702\n",
      "Epoch: 2, Avg Training loss: 0.4843931060228775, Avg Validation loss: 0.43669845660527545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 67/67 [07:03<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.4213167900469766, Avg Input variance: 0.0019924546689239903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Validate]: 100%|██████████| 9/9 [00:54<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.410196539428499, Avg Input variance: 0.0019432866894122628\n",
      "Epoch: 3, Avg Training loss: 0.4213167900469766, Avg Validation loss: 0.410196539428499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 67/67 [07:48<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3989711372709986, Avg Input variance: 0.0019936220626583074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Validate]: 100%|██████████| 9/9 [01:03<00:00,  7.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.38926118943426347, Avg Input variance: 0.0019406687675250901\n",
      "Epoch: 4, Avg Training loss: 0.3989711372709986, Avg Validation loss: 0.38926118943426347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 67/67 [08:02<00:00,  7.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3887824967725953, Avg Input variance: 0.0019924996208760945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Validate]: 100%|██████████| 9/9 [01:00<00:00,  6.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3889416688018375, Avg Input variance: 0.00194791277560095\n",
      "Epoch: 5, Avg Training loss: 0.3887824967725953, Avg Validation loss: 0.3889416688018375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 67/67 [07:22<00:00,  6.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3840321213451784, Avg Input variance: 0.001994121675518578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Validate]: 100%|██████████| 9/9 [00:53<00:00,  5.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37927401396963334, Avg Input variance: 0.0019545620824727747\n",
      "Epoch: 6, Avg Training loss: 0.3840321213451784, Avg Validation loss: 0.37927401396963334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 67/67 [07:02<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3815356928910782, Avg Input variance: 0.001992264986663723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Validate]: 100%|██████████| 9/9 [00:54<00:00,  6.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.38393472962909275, Avg Input variance: 0.001959556571414901\n",
      "Epoch: 7, Avg Training loss: 0.3815356928910782, Avg Validation loss: 0.38393472962909275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 67/67 [07:05<00:00,  6.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.3804243304836216, Avg Input variance: 0.0019939909793380926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Validate]: 100%|██████████| 9/9 [00:53<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.3767971032195621, Avg Input variance: 0.0019407485249555772\n",
      "Epoch: 8, Avg Training loss: 0.3804243304836216, Avg Validation loss: 0.3767971032195621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 67/67 [07:02<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.37971982982621266, Avg Input variance: 0.0019933937113982307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Validate]: 100%|██████████| 9/9 [00:54<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.38269079393810695, Avg Input variance: 0.0019463516720053223\n",
      "Epoch: 9, Avg Training loss: 0.37971982982621266, Avg Validation loss: 0.38269079393810695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 67/67 [07:03<00:00,  6.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training loss: 0.37950242544288065, Avg Input variance: 0.0019920306582587644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Validate]: 100%|██████████| 9/9 [00:53<00:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Validation loss: 0.37617361214425826, Avg Input variance: 0.0019529778769032822\n",
      "Epoch: 10, Avg Training loss: 0.37950242544288065, Avg Validation loss: 0.37617361214425826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "item_encoder_instance = user_CAE(input_dim = (5,10))\n",
    "item_decoder_instance = user_decoder(output_dim = (5,10))\n",
    "\n",
    "# move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "item_encoder_instance.to(device)\n",
    "item_decoder_instance.to(device)\n",
    "\n",
    "# create dataset and dataloader\n",
    "item_train_dataloader, item_validation_dataloader, item_test_dataloader = create_datasets_and_loaders(business_dataset, business_lookup, loaded_model, \"business\", batch_size=128)\n",
    "\n",
    "\n",
    "# train the model\n",
    "training_stats = train_model(item_encoder_instance, item_decoder_instance, item_train_dataloader, item_validation_dataloader, device, \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_datas\\\\item_training_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(training_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Test loss: 0.3736098110675812, Avg Input variance: 0.0012796026421710849\n",
      "Avg Test loss: 0.37574903832541573, Avg Input variance: 0.0013512691819212502\n"
     ]
    }
   ],
   "source": [
    "# Obtain model performance on test set\n",
    "\n",
    "# load the best model\n",
    "user_encoder_instance = user_CAE(input_dim = (5,10))    \n",
    "user_decoder_instance = user_decoder(output_dim = (5,10))\n",
    "\n",
    "user_encoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\word2vec_model_50_5_best_encoder_model.pth\"))\n",
    "user_decoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\word2vec_model_50_5_best_decoder_model.pth\"))\n",
    "\n",
    "# move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "user_encoder_instance.to(device)\n",
    "user_decoder_instance.to(device)\n",
    "\n",
    "# set the model to evaluation mode\n",
    "user_encoder_instance.eval()\n",
    "user_decoder_instance.eval()\n",
    "\n",
    "# create test dataloader\n",
    "loaded_model = Word2Vec.load(\"word2vec_model/word2vec_model_50_5.model\")\n",
    "_, _, user_test_dataloader = create_datasets_and_loaders(user_dataset, user_lookup, loaded_model, \"user\", batch_size=64)\n",
    "\n",
    "# using user_test_dataloader to get the test loss\n",
    "total_test_loss = 0\n",
    "total_test_variance = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_matrix in user_test_dataloader:\n",
    "        user_matrix = user_matrix.to(device).float()\n",
    "\n",
    "        # Compute variance of user_matrix\n",
    "        variance = user_matrix.var()\n",
    "        total_test_variance += variance.item()\n",
    "\n",
    "        # forward\n",
    "        user_embedding = user_encoder_instance(user_matrix)\n",
    "        user_output = user_decoder_instance(user_embedding)\n",
    "        user_loss = reconstruction_loss_with_regularization(user_output, user_matrix, user_encoder_instance)\n",
    "        total_test_loss += user_loss.item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(user_test_dataloader)\n",
    "avg_test_variance = total_test_variance / len(user_test_dataloader)\n",
    "print(f\"Avg Test loss: {avg_test_loss}, Avg Input variance: {avg_test_variance}\")\n",
    "\n",
    "# load the best model\n",
    "item_encoder_instance = user_CAE(input_dim = (5,10))\n",
    "item_decoder_instance = user_decoder(output_dim = (5,10))\n",
    "\n",
    "item_encoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\item_best_encoder_model.pth\"))\n",
    "item_decoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\item_best_decoder_model.pth\"))\n",
    "\n",
    "# move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "item_encoder_instance.to(device)\n",
    "item_decoder_instance.to(device)\n",
    "\n",
    "# set the model to evaluation mode\n",
    "item_encoder_instance.eval()\n",
    "item_decoder_instance.eval()\n",
    "\n",
    "# create test dataloader\n",
    "_, _, item_test_dataloader = create_datasets_and_loaders(business_dataset, business_lookup, loaded_model, \"business\", batch_size=128)\n",
    "\n",
    "# using item_test_dataloader to get the test loss\n",
    "total_test_loss = 0\n",
    "total_test_variance = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for item_matrix in item_test_dataloader:\n",
    "        item_matrix = item_matrix.to(device).float()\n",
    "\n",
    "        # Compute variance of item_matrix\n",
    "        variance = item_matrix.var()\n",
    "        total_test_variance += variance.item()\n",
    "\n",
    "        # forward\n",
    "        item_embedding = item_encoder_instance(item_matrix)\n",
    "        item_output = item_decoder_instance(item_embedding)\n",
    "        item_loss = reconstruction_loss_with_regularization(item_output, item_matrix, item_encoder_instance)\n",
    "        total_test_loss += item_loss.item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(item_test_dataloader)\n",
    "avg_test_variance = total_test_variance / len(item_test_dataloader)\n",
    "print(f\"Avg Test loss: {avg_test_loss}, Avg Input variance: {avg_test_variance}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1 with window - 10\n",
    "1. Avg Test loss: 0.37445858120918274, Avg Input variance: 0.001936642124994912\n",
    "2. Avg Test loss: 0.3762293689780765, Avg Input variance: 0.0020360151279924642\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining embeddings for all user_ids and item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a new Dataset and DataLoader which contains all the users and businesses\n",
    "\n",
    "full_user_dataset = UserDataset(user_dataset,list(user_lookup.keys()),user_lookup,\n",
    "                                w2vmodel=loaded_model)\n",
    "\n",
    "full_business_dataset = BusinessDataset(business_dataset,list(business_lookup.keys()),\n",
    "                                        business_lookup,w2vmodel=loaded_model)\n",
    "\n",
    "full_user_dataloader = DataLoader(full_user_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "full_business_dataloader = DataLoader(full_business_dataset, batch_size=128, shuffle=False, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the embeddings for all users and businesses\n",
    "\n",
    "# load the best model\n",
    "user_encoder_instance = user_CAE(input_dim = (5,10))\n",
    "user_decoder_instance = user_decoder(output_dim = (5,10))\n",
    "\n",
    "user_encoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\word2vec_model_50_5_best_encoder_model.pth\"))\n",
    "user_decoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\word2vec_model_50_5_best_decoder_model.pth\"))\n",
    "\n",
    "# move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "user_encoder_instance.to(device)\n",
    "user_decoder_instance.to(device)\n",
    "\n",
    "# set the model to evaluation mode\n",
    "user_encoder_instance.eval()\n",
    "\n",
    "# create user_embeddings\n",
    "user_embeddings = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_matrix, user_id in full_user_dataloader:\n",
    "        user_matrix = user_matrix.to(device).float()\n",
    "\n",
    "        # forward\n",
    "        user_embedding = user_encoder_instance(user_matrix)\n",
    "        \n",
    "        # iterate over the batch\n",
    "        for id, embedding in zip(user_id, user_embedding):\n",
    "            # move the embedding to cpu and convert to numpy array\n",
    "            embedding = embedding.cpu().numpy()\n",
    "\n",
    "            # add the embedding to the dictionary\n",
    "            user_embeddings[id.item()] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_encoder_instance = user_CAE(input_dim = (5,10))\n",
    "\n",
    "item_encoder_instance.load_state_dict(torch.load(\"AE_model_weights\\\\item_best_encoder_model.pth\"))\n",
    "\n",
    "# move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "item_encoder_instance.to(device)\n",
    "\n",
    "# set the model to evaluation mode\n",
    "item_encoder_instance.eval()\n",
    "\n",
    "# create item_embeddings\n",
    "item_embeddings = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for item_matrix, item_id in full_business_dataloader:\n",
    "        item_matrix = item_matrix.to(device).float()\n",
    "\n",
    "        # forward\n",
    "        item_embedding = item_encoder_instance(item_matrix)\n",
    "        \n",
    "        # iterate over the batch\n",
    "        for id, embedding in zip(item_id, item_embedding):\n",
    "            # move the embedding to cpu and convert to numpy array\n",
    "            embedding = embedding.cpu().numpy()\n",
    "\n",
    "            # add the embedding to the dictionary\n",
    "            item_embeddings[id.item()] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_item_embeddings\\\\user_embeddings.pkl','wb') as f:\n",
    "    pickle.dump(user_embeddings,f)\n",
    "\n",
    "with open('user_item_embeddings\\\\item_embeddings.pkl','wb') as f:\n",
    "    pickle.dump(item_embeddings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
